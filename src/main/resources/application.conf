task {
   log-level = INFO
   log-level = ${?LOG_LEVEL}

   # The parallelism of an individual operator, data source, or data sink can be set using parallelism property
   parallelism = 1
   parallelism = ${?PARALLELISM}

   restart-strategy = fixed-delay
   restart-strategy = ${?TASK_RESTART_STRATEGY}

   use-local-mode = ${?USE_LOCAL_MODE}
   local-mode-port = ${?LOCAL_MODE_PORT}

   state-checkpoints {
      #the interval in milliseconds between checkpoint operations.
      interval = 50000
      interval = ${?CHECK_POINTING_INTERVAL}

      #the minimal pause between checkpointing attempts.
      pause = 500
      pause = ${?CHECK_POINTING_PAUSE}

      #the maximum time that a checkpoint may take before being discarded.
      timeout = 60000
      timeout = ${?CHECK_POINTING_TIMEOUT}

      # path that is used to persist upon checkpoints to guard against data loss and recover consistently.
      # e.g For running locally "file://" protocol and tmp folder can be used and for server "s3://" should be used
      path = "file:///tmp"
      path = ${?CHECK_POINTING_PATH}

      # enable asynchronous snapshots for check pointing
      async = true
      async = ${?CHECK_POINTING_ASYNC}

      incremental = true
      incremental = ${?CHECK_POINTING_ASYNC}
   }
}

input {
   # The Kafka topic to which data will be streamed
   topic = "inputtopic"
   topic = ${?INPUT_TOPIC}

   # Unique string that identifies the consumer group a consumer belongs to
   consumer-group-id = flinkkafkaexample
   consumer-group-id = ${?KAFKA_CONSUMER_GROUP_ID}

   # What to do when there is no initial offset in Kafka or if the current offset does not exist any more on the server
   #  (e.g. because that data has been deleted):
   #  earliest: automatically reset the offset to the earliest offset
   #  latest: automatically reset the offset to the latest offset
   start-from-latest = false
   start-from-latest = ${?KAFKA_CONSUMER_START_FROM_LATEST}
}

output {
   # The Kafka topic to which data in proto format will be output
   topic = "outputtopic"
   topic = ${?OUTPUT_TOPIC}

   # The total memory (in bytes) that the producer can use to buffer records to be sent to the broker.
   # The Producer blocks up to max.block.ms if buffer. memory is exceeded.
   buffer-memory = 50000
   buffer-memory = ${?KAFKA_BUFFER_MEMORY}

   # linger-ms reduces the number of requests so that the producer will wait for up to the given delay to allow other
   # records to be sent
   linger-ms = 1
   linger-ms = ${?KAFKA_LINGER_MS}

   # ack = all ensures the producer gets an ack when all in-sync replicas have received the record.
   ack = all
   ack = ${?KAFKA_ACK}

   write-timestamp = false
   transaction-timeout-ms = ${?PRODUCER_TX_TIMEOUT_MS}
   transactional-id = "flink-kafka-transactional-id"
}

kafka {
   # host represents the host on which kafka server would run
   host = "localhost"
   host = ${?KAFKA_HOST}
   # port represents the port on which kafka server would run
   port = 9092
   port = ${?KAFKA_PORT}
}

log-execution-plan = ${?LOG_EXECUTION_PLAN}